"""
ä» Excel æ–‡ä»¶å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
æ”¯æŒå•ä¸ªæˆ–å¤šä¸ª Excel æ–‡ä»¶çš„å¯¼å…¥
"""
import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import pymysql
import pandas as pd
from colorlog import ColoredFormatter
from tqdm import tqdm

from config import get_db_config


def setup_logging():
    """é…ç½®å½©è‰²æ—¥å¿—"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    handler = logging.StreamHandler()
    handler.setLevel(logging.INFO)

    formatter = ColoredFormatter(
        '%(log_color)s%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'red,bg_white',
        }
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger


class ExcelImporter:
    """Excel æ•°æ®å¯¼å…¥å™¨"""

    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.logger = setup_logging()
        self.connection = None

    def connect(self):
        """è¿æ¥åˆ°æ•°æ®åº“"""
        try:
            self.connection = pymysql.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )
            self.logger.info(f"âœ… å·²è¿æ¥åˆ° MariaDB: {self.db_config['host']}:{self.db_config['port']}")
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            self.logger.info("ğŸ“Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def use_database(self):
        """åˆ‡æ¢åˆ°æŒ‡å®šæ•°æ®åº“"""
        try:
            db_name = self.db_config['database']
            with self.connection.cursor() as cursor:
                cursor.execute(f"USE `{db_name}`")
                self.logger.info(f"ğŸ“ ä½¿ç”¨æ•°æ®åº“: {db_name}")
        except Exception as e:
            self.logger.error(f"âŒ åˆ‡æ¢æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def read_excel(self, file_path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:
        """
        è¯»å– Excel æ–‡ä»¶

        Args:
            file_path: Excel æ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨

        Returns:
            DataFrame
        """
        try:
            if sheet_name:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
            else:
                df = pd.read_excel(file_path)

            self.logger.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path} ({len(df)} è¡Œ, {len(df.columns)} åˆ—)")
            return df
        except Exception as e:
            self.logger.error(f"âŒ è¯»å– Excel æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def validate_columns(self, df: pd.DataFrame, table_name: str) -> bool:
        """
        éªŒè¯ DataFrame åˆ—æ˜¯å¦ä¸æ•°æ®åº“è¡¨åŒ¹é…

        Args:
            df: è¦éªŒè¯çš„ DataFrame
            table_name: ç›®æ ‡è¡¨å

        Returns:
            æ˜¯å¦éªŒè¯é€šè¿‡
        """
        try:
            with self.connection.cursor() as cursor:
                # è·å–è¡¨ç»“æ„
                cursor.execute(f"DESCRIBE `{table_name}`")
                table_columns = set(row['Field'] for row in cursor.fetchall())

            # DataFrame åˆ—åï¼ˆè½¬ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºæ ¼ï¼‰
            df_columns = set(str(col).strip() for col in df.columns)

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åŒ¹é…çš„åˆ—
            missing_in_table = df_columns - table_columns
            missing_in_df = table_columns - df_columns

            if missing_in_table:
                self.logger.warning(f"âš ï¸ DataFrame ä¸­å­˜åœ¨è¡¨ä¸­æ²¡æœ‰çš„åˆ—: {missing_in_table}")

            if missing_in_df:
                self.logger.warning(f"âš ï¸ è¡¨ä¸­å­˜åœ¨ DataFrame ä¸­æ²¡æœ‰çš„åˆ— (å°†ä½¿ç”¨é»˜è®¤å€¼): {missing_in_df}")

            # è‡³å°‘éœ€è¦æœ‰éƒ¨åˆ†åˆ—åŒ¹é…
            common_columns = df_columns & table_columns
            if not common_columns:
                self.logger.error(f"âŒ æ²¡æœ‰åŒ¹é…çš„åˆ—ï¼Œæ— æ³•å¯¼å…¥")
                return False

            self.logger.info(f"âœ… åˆ—éªŒè¯é€šè¿‡ï¼ŒåŒ¹é…åˆ—: {len(common_columns)} ä¸ª")
            return True

        except Exception as e:
            self.logger.error(f"âŒ åˆ—éªŒè¯å¤±è´¥: {e}")
            return False

    def import_dataframe(
        self,
        df: pd.DataFrame,
        table_name: str,
        batch_size: int = 1000,
        truncate_first: bool = False,
        on_duplicate: str = 'skip'
    ):
        """
        å¯¼å…¥ DataFrame åˆ°æ•°æ®åº“è¡¨

        Args:
            df: è¦å¯¼å…¥çš„ DataFrame
            table_name: ç›®æ ‡è¡¨å
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
            truncate_first: å¯¼å…¥å‰æ˜¯å¦æ¸…ç©ºè¡¨
            on_duplicate: é‡åˆ°é‡å¤é”®æ—¶çš„å¤„ç†æ–¹å¼ ('skip', 'update', 'ignore')
        """
        try:
            # è·å–è¡¨çš„å®é™…åˆ—
            with self.connection.cursor() as cursor:
                cursor.execute(f"DESCRIBE `{table_name}`")
                table_columns = [row['Field'] for row in cursor.fetchall()]

            # è¿‡æ»¤å‡ºè¡¨ä¸­å­˜åœ¨çš„åˆ—
            valid_columns = [col for col in df.columns if str(col).strip() in table_columns]
            df_filtered = df[valid_columns].copy()

            # é‡å‘½ååˆ—ï¼ˆå»é™¤ç©ºæ ¼ï¼‰
            df_filtered.columns = [str(col).strip() for col in df_filtered.columns]

            # æ¸…ç©ºè¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if truncate_first:
                with self.connection.cursor() as cursor:
                    cursor.execute(f"TRUNCATE TABLE `{table_name}`")
                    self.connection.commit()
                self.logger.info(f"ğŸ—‘ï¸ è¡¨ '{table_name}' å·²æ¸…ç©º")

            # å‡†å¤‡æ’å…¥æ•°æ®
            total_rows = len(df_filtered)
            columns = df_filtered.columns.tolist()
            placeholders = ', '.join(['%s'] * len(columns))

            # æ ¹æ® on_duplicate å‚æ•°å†³å®š SQL è¯­å¥
            if on_duplicate == 'update':
                # MySQL çš„ ON DUPLICATE KEY UPDATE è¯­æ³•
                update_clause = ', '.join([f"`{col}`=VALUES(`{col}`)" for col in columns])
                sql = f"""
                    INSERT INTO `{table_name}` ({', '.join([f'`{col}`' for col in columns])})
                    VALUES ({placeholders})
                    ON DUPLICATE KEY UPDATE {update_clause}
                """
            elif on_duplicate == 'ignore':
                sql = f"""
                    INSERT IGNORE INTO `{table_name}` ({', '.join([f'`{col}`' for col in columns])})
                    VALUES ({placeholders})
                """
            else:  # skip (é»˜è®¤ï¼Œé‡åˆ°é‡å¤ä¼šæŠ¥é”™è·³è¿‡)
                sql = f"""
                    INSERT INTO `{table_name}` ({', '.join([f'`{col}`' for col in columns])})
                    VALUES ({placeholders})
                """

            # æ‰¹é‡æ’å…¥
            self.logger.info(f"ğŸ”„ å¼€å§‹å¯¼å…¥æ•°æ®åˆ°è¡¨ '{table_name}' ({total_rows} è¡Œ)...")

            success_count = 0
            error_count = 0

            with tqdm(total=total_rows, desc=f"å¯¼å…¥ {table_name}", unit="è¡Œ") as pbar:
                for i in range(0, total_rows, batch_size):
                    batch = df_filtered.iloc[i:i + batch_size]

                    # å¤„ç† NaN å€¼ï¼ˆè½¬ä¸º NULLï¼‰
                    batch_clean = batch.where(pd.notnull(batch), None)

                    # è½¬æ¢ä¸ºåˆ—è¡¨
                    values = [tuple(row) for row in batch_clean.values]

                    try:
                        with self.connection.cursor() as cursor:
                            cursor.executemany(sql, values)
                            self.connection.commit()
                            success_count += len(values)
                            pbar.update(len(values))
                    except Exception as e:
                        self.connection.rollback()
                        error_count += len(values)
                        self.logger.error(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥ (è¡Œ {i}-{i+len(values)}): {e}")

            self.logger.info(f"âœ… å¯¼å…¥å®Œæˆ: æˆåŠŸ {success_count} è¡Œ, å¤±è´¥ {error_count} è¡Œ")

        except Exception as e:
            self.connection.rollback()
            self.logger.error(f"âŒ å¯¼å…¥æ•°æ®å¤±è´¥: {e}")
            raise

    def import_excel_file(
        self,
        file_path: str,
        table_name: Optional[str] = None,
        sheet_name: Optional[str] = None,
        truncate_first: bool = False,
        on_duplicate: str = 'skip',
        batch_size: int = 1000
    ):
        """
        ä» Excel æ–‡ä»¶å¯¼å…¥æ•°æ®

        Args:
            file_path: Excel æ–‡ä»¶è·¯å¾„
            table_name: ç›®æ ‡è¡¨åï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            sheet_name: å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            truncate_first: å¯¼å…¥å‰æ˜¯å¦æ¸…ç©ºè¡¨
            on_duplicate: é‡åˆ°é‡å¤é”®æ—¶çš„å¤„ç†æ–¹å¼ ('skip', 'update', 'ignore')
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
        """
        if not os.path.exists(file_path):
            self.logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¡¨åï¼Œä½¿ç”¨æ–‡ä»¶å
        if table_name is None:
            table_name = Path(file_path).stem

        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ“¥ å¯¼å…¥æ–‡ä»¶: {file_path}")
        self.logger.info(f"ğŸ“‹ ç›®æ ‡è¡¨: {table_name}")
        self.logger.info(f"{'='*60}\n")

        # è¯»å– Excel
        df = self.read_excel(file_path, sheet_name)

        # éªŒè¯åˆ—
        if not self.validate_columns(df, table_name):
            return

        # å¯¼å…¥æ•°æ®
        self.import_dataframe(
            df=df,
            table_name=table_name,
            batch_size=batch_size,
            truncate_first=truncate_first,
            on_duplicate=on_duplicate
        )

    def import_excel_directory(
        self,
        directory: str,
        truncate_first: bool = False,
        on_duplicate: str = 'skip',
        batch_size: int = 1000,
        pattern: str = "*.xlsx"
    ):
        """
        ä»ç›®å½•å¯¼å…¥æ‰€æœ‰ Excel æ–‡ä»¶

        Args:
            directory: Excel æ–‡ä»¶ç›®å½•
            truncate_first: å¯¼å…¥å‰æ˜¯å¦æ¸…ç©ºè¡¨
            on_duplicate: é‡åˆ°é‡å¤é”®æ—¶çš„å¤„ç†æ–¹å¼
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        """
        dir_path = Path(directory)

        if not dir_path.exists():
            self.logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
            return

        excel_files = list(dir_path.glob(pattern))

        if not excel_files:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„ Excel æ–‡ä»¶: {pattern}")
            return

        self.logger.info(f"\nğŸ“ æ‰¾åˆ° {len(excel_files)} ä¸ª Excel æ–‡ä»¶\n")

        for file_path in excel_files:
            self.import_excel_file(
                file_path=str(file_path),
                truncate_first=truncate_first,
                on_duplicate=on_duplicate,
                batch_size=batch_size
            )


def main():
    parser = argparse.ArgumentParser(
        description='ä» Excel æ–‡ä»¶å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å…¥å•ä¸ªæ–‡ä»¶åˆ°åŒåè¡¨
  python import_excel.py data/products.xlsx

  # å¯¼å…¥åˆ°æŒ‡å®šè¡¨
  python import_excel.py data/products.xlsx --table products

  # å¯¼å…¥å‰æ¸…ç©ºè¡¨
  python import_excel.py data/products.xlsx --truncate

  # å¯¼å…¥æ•´ä¸ªç›®å½•
  python import_excel.py --directory data/

  # é‡åˆ°é‡å¤é”®æ—¶æ›´æ–°
  python import_excel.py data/products.xlsx --on-duplicate update

  # æŒ‡å®šå·¥ä½œè¡¨
  python import_excel.py data.xlsx --sheet Sheet1
        """
    )

    parser.add_argument('file', nargs='?', help='Excel æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--table', help='ç›®æ ‡è¡¨åï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰')
    parser.add_argument('--sheet', help='å·¥ä½œè¡¨åç§°ï¼ˆé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰')
    parser.add_argument('--directory', help='å¯¼å…¥æ•´ä¸ªç›®å½•çš„ Excel æ–‡ä»¶')
    parser.add_argument('--truncate', action='store_true', help='å¯¼å…¥å‰æ¸…ç©ºè¡¨')
    parser.add_argument('--on-duplicate', choices=['skip', 'update', 'ignore'],
                        default='skip', help='é‡åˆ°é‡å¤é”®æ—¶çš„å¤„ç†æ–¹å¼')
    parser.add_argument('--batch-size', type=int, default=1000, help='æ‰¹é‡æ’å…¥å¤§å°')
    parser.add_argument('--pattern', default='*.xlsx', help='ç›®å½•æ¨¡å¼åŒ¹é…ï¼ˆé»˜è®¤: *.xlsxï¼‰')

    args = parser.parse_args()

    # è·å–æ•°æ®åº“é…ç½®
    db_config = get_db_config()

    # åˆ›å»ºå¯¼å…¥å™¨
    importer = ExcelImporter(db_config)

    try:
        # è¿æ¥æ•°æ®åº“
        importer.connect()
        importer.use_database()

        # å¯¼å…¥æ•°æ®
        if args.directory:
            # å¯¼å…¥æ•´ä¸ªç›®å½•
            importer.import_excel_directory(
                directory=args.directory,
                truncate_first=args.truncate,
                on_duplicate=args.on_duplicate,
                batch_size=args.batch_size,
                pattern=args.pattern
            )
        elif args.file:
            # å¯¼å…¥å•ä¸ªæ–‡ä»¶
            importer.import_excel_file(
                file_path=args.file,
                table_name=args.table,
                sheet_name=args.sheet,
                truncate_first=args.truncate,
                on_duplicate=args.on_duplicate,
                batch_size=args.batch_size
            )
        else:
            parser.print_help()
            print("\nâŒ è¯·æŒ‡å®šè¦å¯¼å…¥çš„æ–‡ä»¶æˆ–ç›®å½• (--directory)")
            sys.exit(1)

    except Exception as e:
        logging.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        importer.close()


if __name__ == '__main__':
    main()
